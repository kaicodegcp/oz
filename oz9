import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import pyarrow.dataset as ds

file_path = "/appl/hjx_data_p/MRS_AUGMENTED/temp/mrs_augmented_drpsesr_2_0.parquet"

# ---- Schema (as you had) ----
try:
    arrow_schema = pq.read_schema(file_path)
except Exception:
    pf = pq.ParquetFile(file_path)
    arrow_schema = getattr(pf, "schema_arrow", None) or pf.schema.to_arrow_schema()

rows = []
for f in arrow_schema:
    rows.append({
        "Column Name": f.name,
        "Data Type": str(f.type),
        "Nullable": f.nullable,
    })

df_schema = pd.DataFrame(rows, columns=["Column Name", "Data Type", "Nullable"])
pd.set_option("display.max_columns", None)
pd.set_option("display.width", None)
print("\n=== Column Names, Data Types, Nullable ===")
print(df_schema.to_string(index=False))

# ---- First 10 rows ----
print("\n=== First 10 rows ===")
first10_df = None

try:
    # Preferred: stream only what we need
    dataset = ds.dataset(file_path, format="parquet")
    scanner = ds.Scanner.from_dataset(dataset, batch_size=10)  # stream 10 rows
    reader = scanner.to_reader()
    batch = reader.read_next_batch()  # one RecordBatch of up to 10 rows
    first10_df = batch.to_pandas()
except Exception as e1:
    try:
        # Fallback: read then slice (may load more data depending on file/arrow version)
        tbl = pq.read_table(file_path, use_threads=True)
        first10_df = tbl.slice(0, 10).to_pandas()
    except Exception as e2:
        raise RuntimeError(f"Could not read first 10 rows via dataset or fallback: {e1} / {e2}")

print(first10_df.to_string(index=False))




import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import pyarrow.dataset as ds
import pyarrow.compute as pc

file_path = "/appl/hjx_data_p/MRS_AUGMENTED/temp/mrs_augmented_drpsesr_2_0.parquet"

def first_n_where_gse_pls(path, n=10):
    # Try efficient dataset scan with filter (streams only needed rows)
    try:
        dataset = ds.dataset(path, format="parquet")
        filt = (ds.field("gse") == "PLS")
        scanner = ds.Scanner.from_dataset(dataset, filter=filt, batch_size=2048)
        reader = scanner.to_reader()

        batches, total = [], 0
        while total < n:
            try:
                b = reader.read_next_batch()
            except StopIteration:
                break
            if b is None or b.num_rows == 0:
                continue
            batches.append(b)
            total += b.num_rows

        if batches:
            tbl = pa.Table.from_batches(batches).slice(0, n)
            return tbl.to_pandas()

    except Exception:
        pass  # fall back below

    # Fallback: read table then filter (may read more, but always works)
    tbl = pq.read_table(path, use_threads=True)
    mask = pc.equal(tbl["gse"], pa.scalar("PLS"))
    tbl_pls = tbl.filter(mask).slice(0, n)
    return tbl_pls.to_pandas()

df_10 = first_n_where_gse_pls(file_path, n=10)
print(df_10.to_string(index=False))

